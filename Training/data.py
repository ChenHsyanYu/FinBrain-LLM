import os
import json
import pickle
import s3fs
from pathlib import Path
from torch.utils.data import Dataset
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

HF_TOKEN = os.getenv("HF_TOKEN")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_REGION = os.getenv("AWS_REGION")

class S3TextFileDataset(Dataset):
    def __init__(
        self, 
        s3_prefix: str, 
        tokenizer, 
        max_length: int = 2048,
        cache_dir: str = "./tokenized_cache",
        force_reprocess: bool = False
    ):
        """
        å¾ S3 è®€å–æ–‡å­—æª”æ¡ˆä¸¦ tokenizeï¼Œçµæœæœƒå¿«å–åˆ°æœ¬åœ°
        
        Args:
            s3_prefix: S3 è·¯å¾‘å‰ç¶´ (ä¾‹å¦‚: "s3://bucket/path/")
            tokenizer: HuggingFace tokenizer
            max_length: æœ€å¤§ token é•·åº¦
            cache_dir: æœ¬åœ°å¿«å–ç›®éŒ„
            force_reprocess: æ˜¯å¦å¼·åˆ¶é‡æ–°è™•ç†ï¼ˆå¿½ç•¥å¿«å–ï¼‰
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # å»ºç«‹å¿«å–æª”æ¡ˆè·¯å¾‘ï¼ˆåŸºæ–¼ s3_prefix å’Œ max_lengthï¼‰
        cache_key = f"{s3_prefix.replace('/', '_').replace(':', '_')}_{max_length}"
        self.cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        # å¦‚æœå¿«å–å­˜åœ¨ä¸”ä¸å¼·åˆ¶é‡æ–°è™•ç†ï¼Œç›´æ¥è¼‰å…¥
        if self.cache_file.exists() and not force_reprocess:
            print(f"ğŸ“¦ å¾å¿«å–è¼‰å…¥: {self.cache_file}")
            with open(self.cache_file, "rb") as f:
                cache_data = pickle.load(f)
            self.tokenized_data = cache_data["data"]
            self.file_list = cache_data["file_list"]
            print(f"âœ”ï¸ è¼‰å…¥ {len(self.tokenized_data)} ç­†è³‡æ–™ï¼ˆä¾†è‡ª {len(self.file_list)} å€‹æª”æ¡ˆï¼‰")
            return
        
        # å¦å‰‡å¾ S3 è®€å–ä¸¦è™•ç†
        print("ğŸ”„ å¾ S3 è®€å–ä¸¦ tokenize...")
        
        # å¾ç’°å¢ƒè®Šæ•¸è®€å– AWS æ†‘è­‰
        aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
        aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
        aws_region = os.getenv("AWS_REGION", "us-east-1")
        
        if not aws_access_key or not aws_secret_key:
            raise ValueError("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š AWS_ACCESS_KEY_ID å’Œ AWS_SECRET_ACCESS_KEY")
        
        self.fs = s3fs.S3FileSystem(
            key=aws_access_key,
            secret=aws_secret_key,
            client_kwargs={"region_name": aws_region},
        )
        
        if not self.fs.exists(s3_prefix):
            raise ConnectionError(f"ç„¡æ³•å­˜å– S3 prefix: {s3_prefix}")
        
        self.file_list = self._collect_files(s3_prefix)
        if not self.file_list:
            raise FileNotFoundError(f"åœ¨ {s3_prefix} æ²’æœ‰æ‰¾åˆ°æ”¯æ´çš„æª”æ¡ˆ (.txt, .md, .jsonl)")
        
        print(f"âœ”ï¸ æ‰¾åˆ° {len(self.file_list)} å€‹æª”æ¡ˆ")
        
        # é è™•ç†æ‰€æœ‰è³‡æ–™
        self.tokenized_data = self._preprocess_all()
        
        # å­˜æª”
        print(f"ğŸ’¾ å„²å­˜åˆ°: {self.cache_file}")
        cache_data = {
            "data": self.tokenized_data,
            "file_list": self.file_list
        }
        with open(self.cache_file, "wb") as f:
            pickle.dump(cache_data, f)
        
        cache_size_mb = self.cache_file.stat().st_size / (1024 * 1024)
        print(f"âœ”ï¸ å„²å­˜å®Œæˆ ({cache_size_mb:.2f} MB)")
    
    def _collect_files(self, path):
        """ä½¿ç”¨ glob æ”¶é›† S3 è·¯å¾‘ä¸‹æ‰€æœ‰æ”¯æ´çš„æª”æ¡ˆ"""
        collected = []
        
        print(f"ğŸ” é–‹å§‹æƒæ S3 è·¯å¾‘: {path}")
        
        try:
            # æ–¹æ³• 1: ä½¿ç”¨ glob æ¨¡å¼åŒ¹é…ï¼ˆæ¨è–¦ï¼Œæœ€å¿«æœ€å¯é ï¼‰
            # ç¢ºä¿è·¯å¾‘æ ¼å¼æ­£ç¢º
            search_path = path.rstrip('/') + '/'
            
            # åˆ†åˆ¥æœå°‹ä¸‰ç¨®æª”æ¡ˆé¡å‹
            for pattern in ['**/*.txt', '**/*.md', '**/*.jsonl']:
                full_pattern = search_path + pattern
                print(f"   æœå°‹: {pattern}")
                files = self.fs.glob(full_pattern)
                collected.extend(files)
                print(f"   æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆ")
            
            # å»é™¤å¯èƒ½çš„é‡è¤‡
            collected = list(set(collected))
        except Exception as e:
            print(f"âŒ glob æœå°‹å¤±æ•—: {e}")
            print("   å˜—è©¦ä½¿ç”¨ find æ–¹æ³•...")
            
            try:
                # æ–¹æ³• 2: ä½¿ç”¨ findï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
                all_files = self.fs.find(path)
                for file_path in all_files:
                    if file_path.endswith((".txt", ".md", ".jsonl")):
                        collected.append(file_path)
                        if len(collected) % 1000 == 0:
                            print(f"   å·²æ‰¾åˆ° {len(collected)} å€‹æª”æ¡ˆ...")
            except Exception as e2:
                print(f"âŒ find æ–¹æ³•ä¹Ÿå¤±æ•—: {e2}")
                return []
        
        print(f"âœ”ï¸  ç¸½å…±æ‰¾åˆ° {len(collected)} å€‹æª”æ¡ˆ")
        return collected
    
    def _read_file_content(self, file_path):
        """è®€å– S3 æª”æ¡ˆå…§å®¹"""
        try:
            with self.fs.open(file_path, "r", encoding="utf-8") as f:
                if file_path.endswith(".jsonl"):
                    texts = []
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            obj = json.loads(line)
                            if isinstance(obj, dict):
                                text = obj.get("text") or obj.get("content", "")
                                if text:
                                    texts.append(text)
                            else:
                                texts.append(str(obj))
                        except json.JSONDecodeError:
                            continue
                    return "\n".join(texts)
                else:
                    return f.read()
        except Exception as e:
            print(f"âš ï¸  è®€å–éŒ¯èª¤: {file_path}, {e}")
            return ""
    
    def _preprocess_all(self):
        """é è™•ç†æ‰€æœ‰æª”æ¡ˆä¸¦ tokenize"""
        tokenized_data = []
        total_files = len(self.file_list)
        
        for idx, file_path in enumerate(self.file_list):
            # é¡¯ç¤ºé€²åº¦
            if (idx + 1) % 10 == 0 or idx == 0 or idx == total_files - 1:
                print(f"è™•ç†é€²åº¦: {idx + 1}/{total_files} ({(idx + 1) / total_files * 100:.1f}%)")
            
            text = self._read_file_content(file_path)
            if not text.strip():
                print(f"âš ï¸  è·³éç©ºæª”æ¡ˆ: {file_path}")
                continue
            
            tokenized = self.tokenizer(
                text,
                padding="max_length",
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt",
                return_attention_mask=True,
                add_special_tokens=True,
            )
            
            tokenized_data.append({
                "input_ids": tokenized["input_ids"].squeeze(0),
                "attention_mask": tokenized["attention_mask"].squeeze(0),
                "labels": tokenized["input_ids"].squeeze(0),
            })
        
        print(f"âœ”ï¸ æˆåŠŸè™•ç† {len(tokenized_data)} å€‹æª”æ¡ˆ")
        return tokenized_data
    
    def __len__(self):
        return len(self.tokenized_data)
    
    def __getitem__(self, idx):
        return self.tokenized_data[idx]
    
    def clear_cache(self):
        """æ¸…é™¤å¿«å–æª”æ¡ˆ"""
        if self.cache_file.exists():
            self.cache_file.unlink()
            print(f"ğŸ—‘ï¸  å·²åˆªé™¤å¿«å–: {self.cache_file}")


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    from transformers import AutoTokenizer
    
    # è¼‰å…¥ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
            "taide/Llama-3.1-TAIDE-LX-8B-Chat",
            trust_remote_code=True,
            token=HF_TOKEN,
        )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # å»ºç«‹ datasetï¼ˆç¬¬ä¸€æ¬¡æœƒå¾ S3 è™•ç†ï¼Œä¹‹å¾Œæœƒä½¿ç”¨å¿«å–ï¼‰
    dataset = S3TextFileDataset(
        s3_prefix="s3://fin-brain-nccu/clean_text",
        tokenizer=tokenizer,
        max_length=2048,
        cache_dir="./tokenized_cache",
        force_reprocess=False  # è¨­ç‚º True å¯å¼·åˆ¶é‡æ–°è™•ç†
    )
    
    print(f"\nè³‡æ–™é›†å¤§å°: {len(dataset)}")
    
    # å–å¾—ç¬¬ä¸€ç­†è³‡æ–™
    sample = dataset[0]
    print(f"input_ids shape: {sample['input_ids'].shape}")
    print(f"attention_mask shape: {sample['attention_mask'].shape}")
    print(f"labels shape: {sample['labels'].shape}")