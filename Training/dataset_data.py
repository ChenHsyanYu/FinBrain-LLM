import os
import pickle
from tqdm.auto import tqdm
from datasets import load_dataset
from transformers import AutoTokenizer
from dotenv import load_dotenv

# ========== åˆå§‹åŒ– ==========
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

# ========== è¼‰å…¥ Tokenizer ==========
def load_tokenizer():
    print("ğŸ“¥ è¼‰å…¥ Tokenizerï¼štaide/Llama-3.1-TAIDE-LX-8B-Chat")
    tokenizer = AutoTokenizer.from_pretrained(
        "taide/Llama-3.1-TAIDE-LX-8B-Chat",
        trust_remote_code=True,
        token=HF_TOKEN,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


# ========== å»ºç«‹é€šç”¨èªæ–™å¿«å– ==========
def build_general_cache(
    tokenizer, 
    dataset_name="liswei/Taiwan-Text-Excellence-2B",
    split="train",
    max_length=2048,
    cache_dir="./tokenized_cache",
    cache_filename="general_coct2B_2048.pkl",
    sample_ratio=1.0,  # è‹¥è¦åªå–éƒ¨åˆ†èªæ–™ï¼Œå¯è¨­ 0.1 = 10%
):
    os.makedirs(cache_dir, exist_ok=True)
    cache_path = os.path.join(cache_dir, cache_filename)

    if os.path.exists(cache_path):
        print(f"ğŸ“‚ å·²å­˜åœ¨å¿«å–ï¼Œè·³éè™•ç†: {cache_path}")
        return cache_path

    print(f"ğŸŒ ä¸‹è¼‰ Hugging Face èªæ–™: {dataset_name} ({split})")
    dataset = load_dataset(dataset_name, split=split)

    if sample_ratio < 1.0:
        sample_count = int(len(dataset) * sample_ratio)
        dataset = dataset.select(range(sample_count))
        print(f"âœ‚ï¸ å–æ¨£æ¯”ä¾‹: {sample_ratio*100:.1f}% ({sample_count} ç­†æ¨£æœ¬)")

    print(f"ğŸ§© é–‹å§‹ Tokenize {len(dataset):,} ç­†æ¨£æœ¬ï¼ˆmax_length={max_length}ï¼‰...")

    tokenized_data = []
    for i, sample in enumerate(tqdm(dataset, total=len(dataset))):
        text = sample.get("text", "").strip()
        if not text:
            continue

        encoded = tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
            add_special_tokens=True,
        )

        tokenized_data.append({
            "input_ids": encoded["input_ids"].squeeze(0),
            "attention_mask": encoded["attention_mask"].squeeze(0),
            "labels": encoded["input_ids"].squeeze(0),
        })

        # æ¯ 10k ç­†é¡¯ç¤ºä¸€æ¬¡é€²åº¦
        if (i + 1) % 10000 == 0:
            print(f"âœ… å·²å®Œæˆ {i+1:,} ç­†")

    # å„²å­˜åˆ°æœ¬åœ°
    cache_data = {"data": tokenized_data}
    with open(cache_path, "wb") as f:
        pickle.dump(cache_data, f)

    print(f"ğŸ’¾ å·²å„²å­˜é€šç”¨èªæ–™å¿«å– -> {cache_path}")
    print(f"ğŸ“Š ç¸½ç­†æ•¸: {len(tokenized_data):,}")
    cache_size_mb = os.path.getsize(cache_path) / (1024 * 1024)
    print(f"ğŸ“¦ æª”æ¡ˆå¤§å°: {cache_size_mb:.2f} MB")

    return cache_path


if __name__ == "__main__":
    tokenizer = load_tokenizer()
    build_general_cache(tokenizer)
