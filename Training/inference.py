# inference.py
import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from dotenv import load_dotenv

# ========= 1ï¸âƒ£ åˆå§‹åŒ– =========
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

MODEL_PATH = "./models/exp5_pure_finance/checkpoint-364"   # ä½ è‡ªå·±çš„ fine-tuned æ¨¡å‹
BASE_MODEL = "taide/Llama-3.1-TAIDE-LX-8B-Chat"

# ========= 2ï¸âƒ£ GPU æª¢æŸ¥ =========
print("ğŸ” æª¢æŸ¥ GPU ç‹€æ…‹ä¸­...")
if torch.cuda.is_available():
    print(f"âœ… ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    print(f"ğŸ§® ç²¾åº¦: {dtype}")
else:
    print("âš ï¸ æœªåµæ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPUï¼Œé€Ÿåº¦å¯èƒ½æœƒè¼ƒæ…¢ã€‚")
    dtype = torch.float32

# ========= 3ï¸âƒ£ è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer =========
print(f"ğŸ“¥ è¼‰å…¥æ¨¡å‹ä¸­: {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL,
    use_auth_token=HF_TOKEN,
    legacy=False,
)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    torch_dtype=dtype,
    use_auth_token=HF_TOKEN,
)

# ========= 4ï¸âƒ£ å®šç¾©ç”Ÿæˆå‡½å¼ =========
def generate_response(system_prompt, user_prompt, max_new_tokens=512):
    """
    çµ¦å®š system + user promptï¼Œç”Ÿæˆæ¨¡å‹å›ç­”
    """
    chat = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]
    
    prompt = tokenizer.apply_chat_template(
        chat, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # å˜—è©¦åªä¿ç•™ assistant å›ç­”
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()
    return response

# ========= 5ï¸âƒ£ æ¸¬è©¦åŸ·è¡Œ =========
if __name__ == "__main__":
    print("\nğŸ’¬ æ¨¡å‹æ¨è«–æ¸¬è©¦é–‹å§‹ï¼\n")
    
    # å¯è‡ªè¡Œä¿®æ”¹é€™è£¡çš„ prompt æ¸¬è©¦
    system_prompt = "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é‡‘èé¡§å•ï¼Œè«‹ä»¥ç°¡æ½”ã€ç”Ÿæ´»åŒ–çš„æ–¹å¼è§£é‡‹å•é¡Œã€‚"
    user_prompt = ""
    while True:
        user_prompt=input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š")
        output = generate_response(system_prompt, user_prompt)
        print("\nğŸ‘¤ ä½¿ç”¨è€…å•é¡Œï¼š", user_prompt)
        print("\nğŸ¤– æ¨¡å‹å›è¦†ï¼š\n", output)
    
    
    
