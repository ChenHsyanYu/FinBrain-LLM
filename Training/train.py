import os
import json
import warnings
import torch
import torch.nn as nn
import botocore.exceptions
from tqdm.auto import tqdm
from dotenv import load_dotenv
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    logging as hf_logging,
    TrainerCallback
)
from torch.utils.data import Dataset, random_split
import pickle
import math
# chsyu-national-chengchi-university/fin_brain_reset/z8afiu8i
os.environ["WANDB_PROJECT"]="fin_brain_reset"
os.environ["WANDB_RESUME"] = "allow"
os.environ["PYTORCH_HIP_ALLOC_CONF"] = "expandable_segments:True"
# os.environ["WANDB_RUN_ID"] = "z8afiu8i"
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
if "WANDB_RUN_ID" in os.environ:
        del os.environ["WANDB_RUN_ID"]

# ========== åˆå§‹åŒ– ==========
warnings.filterwarnings("ignore")
hf_logging.set_verbosity_error()
load_dotenv()

HF_TOKEN = os.getenv("HF_TOKEN")


# ========== ç³»çµ±æª¢æŸ¥ ==========
def check_system_resources():
    print("ğŸ” ç³»çµ±è³‡æºæª¢æŸ¥:")
    print(f"   PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print(f"   GPU æ•¸é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            print(
                f"   GPU {i} è¨˜æ†¶é«”: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB"
            )

        try:
            bf16_supported = torch.cuda.is_bf16_supported()
            print(f"   BF16: {'âœ… æ”¯æ´' if bf16_supported else 'âŒ ä¸æ”¯æ´'}")
        except:
            print("   BF16: â“ ç„¡æ³•æª¢æ¸¬")

        try:
            _ = torch.randn(10, device="cuda", dtype=torch.float16)
            print("   FP16: âœ… æ”¯æ´")
        except:
            print("   FP16: âŒ ä¸æ”¯æ´")

        if hasattr(torch, "float8_e4m3fn"):
            try:
                _ = torch.randn(10, device="cuda").to(torch.float8_e4m3fn)
                print("   FP8: âœ… æ”¯æ´")
            except Exception as e:
                print(f"   FP8 æ¸¬è©¦å¤±æ•—: {e}")
        else:
            print("   FP8: ä¸æ”¯æ´")

        major, minor = torch.cuda.get_device_capability()
        print(f"   Compute Capability: {major}.{minor}")
    else:
        print("   åƒ… CPU æ¨¡å¼ï¼Œå»ºè­° FP32")


# ========== æ¨¡å‹è¼‰å…¥ ==========
def load_model_and_tokenizer():
    print("ğŸ“¥ è¼‰å…¥ TAIDE æ¨¡å‹...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            "taide/Llama-3.1-TAIDE-LX-8B-Chat",
            device_map=None,
            torch_dtype=torch.bfloat16,
        )
        tokenizer = AutoTokenizer.from_pretrained(
            "taide/Llama-3.1-TAIDE-LX-8B-Chat",
            trust_remote_code=True,
            token=HF_TOKEN,
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None, None


# ========== Dataset ==========
class LocalTensorDataset(Dataset):
    def __init__(self, tensor_file: str, tokenizer=None, max_length: int = 2048):
        print(f"ğŸ“¦ è¼‰å…¥å¿«å–æª”æ¡ˆ: {tensor_file}")
        with open(tensor_file, "rb") as f:
            self.data = pickle.load(f)

        self.tokenized_data = self.data["data"]
        print(f"âœ”ï¸ æˆåŠŸè¼‰å…¥ {len(self.tokenized_data)} ç­†æ¨£æœ¬")

    def __len__(self):
        return len(self.tokenized_data)

    def __getitem__(self, idx):
        sample = self.tokenized_data[idx]
        return {
            "input_ids": sample["input_ids"].to(torch.long),
            "attention_mask": sample["attention_mask"].to(torch.long),
            "labels": sample["labels"].to(torch.long),
        }

# ========== å‡çµåƒæ•¸ ==========
def freeze_model_layers(model, layer: str):
    for _, param in model.named_parameters():
        param.requires_grad = False

    if layer in ["last_transformer", "both"]:
        for name, param in model.named_parameters():
            if "model.layers.31." in name:
                param.requires_grad = True
    if layer in ["lm_head", "both"]:
        for name, param in model.named_parameters():
            if "lm_head" in name:
                param.requires_grad = True

    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"ğŸ”¢ å¯è¨“ç·´åƒæ•¸: {trainable:,} / {total:,} ({trainable/total:.2%})")


# ========== è¨“ç·´è¨­å®š ==========
def setup_training_args():
    name = "exp5_pure_finance"
    
    return TrainingArguments(
        output_dir=f"./models/{name}",
        logging_dir=f"./logs/{name}",
        run_name=name,
        
        # è¨“ç·´é•·åº¦
        num_train_epochs=10,  # å¢åŠ è¨“ç·´æ™‚é–“
        
        # å­¸ç¿’ç‡è¨­ç½® - é—œéµä¿®å¾©
        learning_rate=6e-5,
        lr_scheduler_type="cosine_with_restarts",  # ğŸ‘ˆ æ”¹ç‚ºæ†å®š
        lr_scheduler_kwargs={"num_cycles": 2},
        warmup_steps=20,
        max_grad_norm=1.0,
        
        # å„ªåŒ–å™¨
        optim="adamw_torch",
        adam_beta2=0.95,
        weight_decay=0.1,
        
        # æ··åˆç²¾åº¦
        bf16=True,
        
        # Batch è¨­ç½®
        per_device_train_batch_size=16,
        gradient_accumulation_steps=16,
        
        # è©•ä¼°ç­–ç•¥ - é‡è¦æ·»åŠ 
        eval_strategy="epoch",        # ğŸ‘ˆ æ·»åŠ 
        per_device_eval_batch_size=1,       # æ›´ä¿å®ˆ
        eval_accumulation_steps=1,          # æ¯æ­¥æŠŠæš«å­˜çµæœæ¬èµ°ï¼Œé¿å…å †ç©
        prediction_loss_only=True, 
        
        # ä¿å­˜ç­–ç•¥
        save_strategy="epoch",
        load_best_model_at_end=True,  # ğŸ‘ˆ æ·»åŠ ï¼ˆéœ€è¦é©—è­‰é›†ï¼‰
        metric_for_best_model="eval_loss",  # ğŸ‘ˆ æ·»åŠ 
        
        # æ—¥èªŒ
        logging_strategy="epoch",
        # logging_steps=52,  # ä¸éœ€è¦æ¯æ­¥éƒ½è¨˜éŒ„
        
        # å…¶ä»–
        gradient_checkpointing=False,
        dataloader_pin_memory=True,  # å»ºè­°æ”¹ç‚º True
        dataloader_num_workers=0,    # å¢åŠ  workers
        remove_unused_columns=False,
        report_to="wandb",
    )


def compute_metrics(eval_pred):
    """å›å‚³ perplexityï¼Œç¢ºä¿æœƒè¢« log åˆ° W&B"""
    metrics = eval_pred.metrics
    if "eval_loss" in metrics:
        try:
            ppl = math.exp(metrics["eval_loss"])
        except OverflowError:
            ppl = float("inf")
        metrics["perplexity"] = ppl
    return metrics


class PerplexityCallback(TrainerCallback):
    """åœ¨ console å³æ™‚å°å‡º perplexity"""
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics and "eval_loss" in metrics:
            try:
                ppl = math.exp(metrics["eval_loss"])
            except OverflowError:
                ppl = float("inf")
            print(f"ğŸ“ˆ Step {state.global_step}: eval_loss={metrics['eval_loss']:.4f} | ppl={ppl:.2f}")
        return control


def start_training(model, tokenizer, dataset, training_args):
    if not all([model, tokenizer, dataset]):
        print("âŒ æ¨¡å‹ã€tokenizer æˆ–è³‡æ–™é›†æœªå°±ç·’")
        return

    # ========== å‡çµåƒæ•¸ ==========
    freeze_model_layers(model, "last_transformer")

    # ========== è¼‰å…¥é€šç”¨èªæ–™ä¸¦æŠ½æ¨£ ==========
    # general_path = "tokenized_cache/general_coct2B_2048.pkl"
    # all_samples = list(dataset)  # å…ˆæ”¶é›†é‡‘èèªæ–™

    # if os.path.exists(general_path):
    #     print(f"\nğŸ“¦ è¼‰å…¥é€šç”¨èªæ–™: {general_path}")
    #     with open(general_path, "rb") as f:
    #         general_data = pickle.load(f)
    #     general_samples = general_data["data"]
    #     total_general = len(general_samples)
    #     print(f"   é€šç”¨èªæ–™ç¸½ç­†æ•¸: {total_general:,}")

    #     # âœ¨ æŠ½æ¨£æ¯”ä¾‹ï¼ˆ3%ï¼‰
    #     sample_ratio = 0.03
    #     sample_size = max(1, int(total_general * sample_ratio))
    #     print(f"   æŠ½æ¨£æ¯”ä¾‹: {sample_ratio*100:.1f}% â†’ æŠ½æ¨£ {sample_size:,} ç­†")

    #     import random
    #     random.seed(42)
    #     general_samples = random.sample(general_samples, sample_size)

    #     # è½‰æ›æˆç›¸åŒæ ¼å¼
    #     for s in general_samples:
    #         all_samples.append(
    #             {
    #                 "input_ids": s["input_ids"].to(torch.long),
    #                 "attention_mask": s["attention_mask"].to(torch.long),
    #                 "labels": s["labels"].to(torch.long),
    #             }
    #         )

    #     print(f"âœ… åˆä½µå¾Œè³‡æ–™ç¸½ç­†æ•¸: {len(all_samples):,}")
    # else:
    #     print("âš ï¸ æ‰¾ä¸åˆ° general_coct2B_2048.pklï¼Œç•¥éåˆä½µã€‚")

    # ========== åˆä½µå¾Œå†åˆ‡åˆ† train / validation ==========
    print("ğŸ“Š å¾æ··åˆèªæ–™åˆ‡åˆ†è¨“ç·´èˆ‡é©—è­‰é›†...")
    total_size = len(dataset)
    train_size = int(0.9 * total_size)
    val_size = total_size - train_size

    # å°è£æˆ Dataset ç‰©ä»¶
    class MixedDataset(Dataset):
        def __init__(self, data_list):
            self.data = data_list
        def __len__(self):
            return len(self.data)
        def __getitem__(self, idx):
            return self.data[idx]

    mixed_dataset = MixedDataset(dataset)
    train_dataset, val_dataset = random_split(
        mixed_dataset,
        [train_size, val_size],
        generator=torch.manual_seed(42),
    )

    print(f"   è¨“ç·´é›†: {len(train_dataset):,} ç­†")
    print(f"   é©—è­‰é›†: {len(val_dataset):,} ç­†")

    # ========== å»ºç«‹ Trainer ==========
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        callbacks=[PerplexityCallback()],
    )

    # ========== é–‹å§‹è¨“ç·´ ==========
    print("ğŸ”¥ é–‹å§‹è¨“ç·´...")
    result = trainer.train(resume_from_checkpoint=False)

    print("âœ… è¨“ç·´å®Œæˆ")
    print(f"   æœ€ä½³é©—è­‰ loss: {trainer.state.best_metric:.4f}")

    trainer.save_model()
    print("ğŸ“ æ¨¡å‹å·²ä¿å­˜å®Œç•¢ã€‚")
    return result




if __name__ == "__main__":
    check_system_resources()
    model, tokenizer = load_model_and_tokenizer()
    dataset = LocalTensorDataset("tokenized_cache/s3___fin-brain-nccu_clean_text_2048.pkl", tokenizer)
    training_args = setup_training_args()
    start_training(model, tokenizer, dataset, training_args)

